name: Performance Regression Testing

on:
  pull_request:
    branches: [ master, main ]
  push:
    branches: [ master, main ]
  schedule:
    # Run nightly performance tests
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_filter:
        description: 'Benchmark filter pattern (e.g., "test_hdf5*" or "all")'
        required: false
        default: 'all'
      baseline_update:
        description: 'Update baseline after successful run'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.12'
  PERFORMANCE_TEST_TIMEOUT: 3600  # 1 hour timeout for performance tests

jobs:
  performance-regression-test:
    runs-on: ubuntu-latest
    timeout-minutes: 90

    strategy:
      fail-fast: false
      matrix:
        test-category:
          - "hdf5_performance"
          - "scientific_analysis_performance"
          - "memory_threading_performance"
          - "scalability"

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for trend analysis

    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y \
          libhdf5-dev \
          libffi-dev \
          libblas-dev \
          liblapack-dev \
          gfortran

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        # Install additional performance testing dependencies
        pip install memory_profiler psutil

    - name: Create performance test directories
      run: |
        mkdir -p tests/performance/reports
        mkdir -p tests/performance/data

    - name: Download baseline performance data
      uses: actions/download-artifact@v3
      with:
        name: performance-baseline
        path: tests/performance/config/
      continue-on-error: true

    - name: Run performance benchmarks
      id: benchmark
      run: |
        cd tests/performance

        # Determine test filter
        if [[ "${{ github.event.inputs.benchmark_filter }}" == "" || "${{ github.event.inputs.benchmark_filter }}" == "all" ]]; then
          FILTER_PATTERN="test_${{ matrix.test-category }}.py"
        else
          FILTER_PATTERN="${{ github.event.inputs.benchmark_filter }}"
        fi

        echo "Running benchmarks with pattern: $FILTER_PATTERN"

        # Run benchmarks with timeout and save results
        timeout ${{ env.PERFORMANCE_TEST_TIMEOUT }} python -m pytest \
          benchmarks/$FILTER_PATTERN \
          --benchmark-only \
          --benchmark-json=reports/benchmark_results_${{ matrix.test-category }}.json \
          --benchmark-save=results_${{ matrix.test-category }} \
          --benchmark-save-data \
          --benchmark-compare-fail=mean:20% \
          --benchmark-compare-fail=min:30% \
          --benchmark-histogram=reports/histogram_${{ matrix.test-category }} \
          -v \
          --tb=short || echo "BENCHMARKS_FAILED=true" >> $GITHUB_OUTPUT

    - name: Analyze performance regression
      id: regression-analysis
      run: |
        python -c "
        import sys
        import json
        from pathlib import Path

        # Add performance utilities to path
        sys.path.append('tests/performance')
        from utils.regression_detection import (
            PerformanceRegressionDetector,
            BenchmarkResult,
            run_regression_analysis,
            CIIntegration
        )

        # Load benchmark results
        results_file = Path('tests/performance/reports/benchmark_results_${{ matrix.test-category }}.json')
        if results_file.exists():
            with open(results_file) as f:
                benchmark_data = json.load(f)

            # Convert to BenchmarkResult objects
            results = []
            for benchmark in benchmark_data.get('benchmarks', []):
                result = BenchmarkResult(
                    name=benchmark['name'],
                    mean_time=benchmark['stats']['mean'],
                    std_time=benchmark['stats']['stddev'],
                    min_time=benchmark['stats']['min'],
                    max_time=benchmark['stats']['max'],
                    rounds=benchmark['stats']['rounds']
                )
                results.append(result)

            # Run regression analysis
            baseline_file = Path('tests/performance/config/baseline_performance.json')
            output_dir = Path('tests/performance/reports')

            ci_pass = run_regression_analysis(results, baseline_file, output_dir)

            print(f'REGRESSION_ANALYSIS_PASS={ci_pass}')
            print(f'::set-output name=analysis_pass::{ci_pass}')
            print(f'::set-output name=results_count::{len(results)}')

            # Generate GitHub Actions outputs
            if (output_dir / 'regression_report.json').exists():
                with open(output_dir / 'regression_report.json') as f:
                    report = json.load(f)

                github_output = CIIntegration.generate_github_actions_output(report)
                print(github_output)
        else:
            print('No benchmark results found')
            print('::set-output name=analysis_pass::false')
            print('::set-output name=results_count::0')
        "

    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.test-category }}
        path: |
          tests/performance/reports/benchmark_results_${{ matrix.test-category }}.json
          tests/performance/reports/regression_report.json
          tests/performance/reports/regression_report.md
          tests/performance/reports/regression_results.xml
          tests/performance/reports/histogram_${{ matrix.test-category }}*
        retention-days: 30

    - name: Upload test results to GitHub
      uses: dorny/test-reporter@v1
      if: always()
      with:
        name: Performance Regression Results (${{ matrix.test-category }})
        path: tests/performance/reports/regression_results.xml
        reporter: java-junit

    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'tests/performance/reports/regression_report.md';

          if (fs.existsSync(path)) {
            const report = fs.readFileSync(path, 'utf8');

            // Find existing performance comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const performanceCommentMarker = `<!-- PERFORMANCE-REGRESSION-${{ matrix.test-category }} -->`;
            const existingComment = comments.data.find(comment =>
              comment.body.includes(performanceCommentMarker)
            );

            const commentBody = `${performanceCommentMarker}
          ## Performance Test Results - ${{ matrix.test-category }}

          ${report}

          <details>
          <summary>View benchmark details</summary>

          **Test Category:** ${{ matrix.test-category }}
          **Commit:** ${{ github.sha }}
          **Workflow:** ${{ github.workflow }}
          **Run:** ${{ github.run_id }}

          </details>`;

            if (existingComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: existingComment.id,
                body: commentBody
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: commentBody
              });
            }
          }

    - name: Fail job on critical regressions
      if: steps.regression-analysis.outputs.analysis_pass == 'false'
      run: |
        echo "‚ùå Performance regression check failed"
        echo "Critical performance regressions detected"
        exit 1

  consolidate-results:
    needs: performance-regression-test
    runs-on: ubuntu-latest
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all benchmark results
      uses: actions/download-artifact@v3
      with:
        path: benchmark-artifacts/

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"

    - name: Consolidate performance results
      run: |
        python -c "
        import json
        import sys
        from pathlib import Path
        from datetime import datetime

        sys.path.append('tests/performance')
        from utils.regression_detection import PerformanceTrendAnalyzer

        # Collect all benchmark results
        all_results = {}
        artifact_dir = Path('benchmark-artifacts')

        for category_dir in artifact_dir.iterdir():
            if category_dir.is_dir():
                for result_file in category_dir.glob('benchmark_results_*.json'):
                    category = result_file.stem.replace('benchmark_results_', '')

                    try:
                        with open(result_file) as f:
                            data = json.load(f)
                        all_results[category] = data
                        print(f'Loaded results for {category}: {len(data.get(\"benchmarks\", []))} benchmarks')
                    except Exception as e:
                        print(f'Error loading {result_file}: {e}')

        # Save consolidated results
        consolidated_file = Path('consolidated_performance_results.json')
        with open(consolidated_file, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'commit_hash': '${{ github.sha }}',
                'workflow_run_id': '${{ github.run_id }}',
                'categories': all_results
            }, f, indent=2)

        print(f'Consolidated results saved to {consolidated_file}')
        "

    - name: Upload consolidated results
      uses: actions/upload-artifact@v3
      with:
        name: consolidated-performance-results
        path: consolidated_performance_results.json
        retention-days: 90

    - name: Update performance history
      if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
      run: |
        # This would append results to a historical database
        # For now, we'll just create a timestamped file
        HISTORY_FILE="performance_history_$(date +%Y%m%d_%H%M%S).json"
        cp consolidated_performance_results.json "$HISTORY_FILE"
        echo "Performance history updated: $HISTORY_FILE"

    - name: Update baseline on successful run
      if: |
        (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main') &&
        (github.event.inputs.baseline_update == 'true' || github.event_name == 'schedule') &&
        success()
      run: |
        python -c "
        import json
        import sys
        from pathlib import Path

        sys.path.append('tests/performance')
        from utils.regression_detection import PerformanceRegressionDetector, BenchmarkResult

        # Load consolidated results and update baseline
        consolidated_file = Path('consolidated_performance_results.json')
        if consolidated_file.exists():
            with open(consolidated_file) as f:
                data = json.load(f)

            # Convert to BenchmarkResult objects
            all_results = []
            for category, category_data in data['categories'].items():
                for benchmark in category_data.get('benchmarks', []):
                    result = BenchmarkResult(
                        name=benchmark['name'],
                        mean_time=benchmark['stats']['mean'],
                        std_time=benchmark['stats']['stddev'],
                        min_time=benchmark['stats']['min'],
                        max_time=benchmark['stats']['max'],
                        rounds=benchmark['stats']['rounds']
                    )
                    all_results.append(result)

            # Update baseline
            detector = PerformanceRegressionDetector()
            baseline_file = Path('tests/performance/config/baseline_performance.json')
            detector.save_baseline(all_results, baseline_file)

            print(f'Baseline updated with {len(all_results)} benchmark results')
        "

    - name: Upload updated baseline
      if: |
        (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main') &&
        (github.event.inputs.baseline_update == 'true' || github.event_name == 'schedule')
      uses: actions/upload-artifact@v3
      with:
        name: performance-baseline
        path: tests/performance/config/baseline_performance.json
        retention-days: 365

  performance-report:
    needs: consolidate-results
    runs-on: ubuntu-latest
    if: always() && (github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main')

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download consolidated results
      uses: actions/download-artifact@v3
      with:
        name: consolidated-performance-results
        path: ./

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install matplotlib seaborn  # For report generation

    - name: Generate performance dashboard
      run: |
        python -c "
        import json
        import matplotlib.pyplot as plt
        import seaborn as sns
        from pathlib import Path

        # Load results
        with open('consolidated_performance_results.json') as f:
            data = json.load(f)

        # Generate simple performance summary
        total_benchmarks = 0
        categories_summary = {}

        for category, category_data in data['categories'].items():
            benchmarks = category_data.get('benchmarks', [])
            total_benchmarks += len(benchmarks)

            # Calculate average performance
            if benchmarks:
                avg_time = sum(b['stats']['mean'] for b in benchmarks) / len(benchmarks)
                categories_summary[category] = {
                    'count': len(benchmarks),
                    'avg_time': avg_time
                }

        # Create simple bar chart
        fig, ax = plt.subplots(figsize=(12, 6))
        categories = list(categories_summary.keys())
        counts = [categories_summary[cat]['count'] for cat in categories]

        ax.bar(categories, counts)
        ax.set_title('Performance Benchmarks by Category')
        ax.set_ylabel('Number of Benchmarks')
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig('performance_summary.png', dpi=150, bbox_inches='tight')
        plt.close()

        print(f'Generated performance dashboard for {total_benchmarks} benchmarks')
        "

    - name: Deploy to GitHub Pages
      if: github.ref == 'refs/heads/master'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./
        publish_branch: gh-pages
        destination_dir: performance-reports
        keep_files: true

    - name: Create GitHub Release for Performance Data
      if: github.event_name == 'schedule'
      uses: actions/create-release@v1
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      with:
        tag_name: performance-data-${{ github.run_number }}
        release_name: Performance Data $(date +%Y-%m-%d)
        body: |
          Automated performance benchmark results

          **Commit:** ${{ github.sha }}
          **Timestamp:** $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Workflow Run:** ${{ github.run_id }}

          This release contains comprehensive performance benchmark results for XPCS Toolkit.
        draft: false
        prerelease: false

# Performance test environment setup
env:
  PYTHONPATH: ${{ github.workspace }}
  PERFORMANCE_DATA_DIR: ${{ github.workspace }}/tests/performance/data
  PERFORMANCE_REPORTS_DIR: ${{ github.workspace }}/tests/performance/reports
  # Optimize for consistent performance measurements
  OMP_NUM_THREADS: 1
  PYTHONHASHSEED: 0
  # Suppress Qt warnings that might affect benchmark timing
  PYXPCS_SUPPRESS_QT_WARNINGS: 1